{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed00fc0-27a9-47ab-bd9f-3f7877b365a0",
   "metadata": {},
   "source": [
    "# Q1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a884886-9e9b-483e-aa5a-99e6e36f8ad1",
   "metadata": {},
   "source": [
    "Lasso is a modification of linear regression, where the model is penalized for the sum of absolute values of the weights. Thus, the absolute values of weight will be (in general) reduced, and many will tend to be zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11646c92-4c92-45c1-b1b0-f9edd08d2b1f",
   "metadata": {},
   "source": [
    "# Q2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53c6af2-f596-4104-a306-376923b58020",
   "metadata": {},
   "source": [
    "The main advantage of a LASSO regression model is that it has the ability to set the coefficients for features it does not consider interesting to zero. This means that the model does some automatic feature selection to decide which features should and should not be included on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bdd035-0903-4b6f-8a1a-26d7692e9089",
   "metadata": {},
   "source": [
    "# Q3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb0882-e4f1-4e82-9949-c5fa97cd2e86",
   "metadata": {},
   "source": [
    "The estimated coefficients target the same targets, and both have some estimation error (which, if squared, can be decomposed into bias and variance), so in this sense their interpretation is the same. Now of course the methods are not the same, so you get different estimated coefficient values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7004a06-2cc0-4208-a0b9-412ee2f9e0b3",
   "metadata": {},
   "source": [
    "# Q4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b700e99e-db6e-42a8-8976-785c809ced7e",
   "metadata": {},
   "source": [
    "A tuning parameter (Î»), sometimes called a penalty parameter, controls the strength of the penalty term in ridge regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a central point, like the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71769b5-d60d-4a5e-aaca-397f0b1e5d38",
   "metadata": {},
   "source": [
    "# Q5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a91f42a-9096-4dbc-867d-4df17be14d35",
   "metadata": {},
   "source": [
    "Regularization with a lasso penalty is an advantageous in that it estimates some coefficients in linear regression models to be exactly zero. We propose imposing a weighted lasso penalty on a nonlinear regression model and thereby selecting the number of basis functions effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a2672-d3bc-4370-9b8a-89751e77581c",
   "metadata": {},
   "source": [
    "# Q6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb17a54-e031-4839-a4b3-8f0947eb0f0b",
   "metadata": {},
   "source": [
    "Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square. Ridge regression is also referred to as L2 Regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d36f0-18db-4f4c-80ea-df3b25f5a78f",
   "metadata": {},
   "source": [
    "# Q7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a90a0-6e5b-49d1-8c7d-76798c37439a",
   "metadata": {},
   "source": [
    "Finally, LASSO regression is useful when you have some multicollinearity in your model. Multicollinearity means that the predictors variables, also known as independent variables, aren't so independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5756e65c-536b-4657-9795-de4ea26905b0",
   "metadata": {},
   "source": [
    "# Q8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7811432e-4af5-40e8-8ca3-5dfbadd1eaa8",
   "metadata": {},
   "source": [
    "When choosing a lambda value, the goal is to strike the right balance between simplicity and training-data fit: If your lambda value is too high, your model will be simple, but you run the risk of underfitting your data. Your model won't learn enough about the training data to make useful predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
