{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1f3be4-c0a2-4e2a-8f96-221d1faa1618",
   "metadata": {},
   "source": [
    "## Q1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b460c9af-c4e8-4aba-b74c-a9c3952cf08e",
   "metadata": {},
   "source": [
    "R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. In other words, r-squared shows how well the data fit the regression model (the goodness of fit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0561f-ab9e-4b21-9bc1-f7a835de18d2",
   "metadata": {},
   "source": [
    "# Q2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf55d8-9a3b-4116-9e82-0414e8e6d942",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2197a34e-179d-474a-9935-7b275b871f05",
   "metadata": {},
   "source": [
    "# Q3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91049f8f-b5be-4132-8e9e-d231edcb8856",
   "metadata": {},
   "source": [
    "Clearly, it is better to use Adjusted R-squared when there are multiple variables in the regression model. This would allow us to compare models with differing numbers of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94af601-82e4-4b24-adc4-08c1f55dfa4e",
   "metadata": {},
   "source": [
    "# Q4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f74e934-319f-469d-abc3-6a4130f0771c",
   "metadata": {},
   "source": [
    "The essential step in any machine learning model is to evaluate the accuracy of the model. The Mean Squared Error, Mean absolute error, Root Mean Squared Error, and R-Squared or Coefficient of determination metrics are used to evaluate the performance of the model in regression analysis.\n",
    "\n",
    "The Mean absolute error represents the average of the absolute difference between the actual and predicted values in the dataset. It measures the average of the residuals in the dataset.\n",
    "\n",
    "\n",
    "Mean Squared Error represents the average of the squared difference between the original and predicted values in the data set. It measures the variance of the residuals.\n",
    "\n",
    "Root Mean Squared Error is the square root of Mean Squared error. It measures the standard deviation of residuals.\n",
    "\n",
    "The coefficient of determination or R-squared represents the proportion of the variance in the dependent variable which is explained by the linear regression model. It is a scale-free score i.e. irrespective of the values being small or large, the value of R square will be less than one.\n",
    "\n",
    "Adjusted R squared is a modified version of R square, and it is adjusted for the number of independent variables in the model, and it will always be less than or equal to R².In the formula below n is the number of observations in the data and k is the number of the independent variables in the data.\n",
    "\n",
    "Differences among these evaluation metrics\n",
    "Mean Squared Error(MSE) and Root Mean Square Error penalizes the large prediction errors vi-a-vis Mean Absolute Error (MAE). However, RMSE is widely used than MSE to evaluate the performance of the regression model with other random models as it has the same units as the dependent variable (Y-axis).\n",
    "MSE is a differentiable function that makes it easy to perform mathematical operations in comparison to a non-differentiable function like MAE. Therefore, in many models, RMSE is used as a default metric for calculating Loss Function despite being harder to interpret than MAE.\n",
    "The lower value of MAE, MSE, and RMSE implies higher accuracy of a regression model. However, a higher value of R square is considered desirable.\n",
    "R Squared & Adjusted R Squared are used for explaining how well the independent variables in the linear regression model explains the variability in the dependent variable. R Squared value always increases with the addition of the independent variables which might lead to the addition of the redundant variables in our model. However, the adjusted R-squared solves this problem.\n",
    "Adjusted R squared takes into account the number of predictor variables, and it is used to determine the number of independent variables in our model. The value of Adjusted R squared decreases if the increase in the R square by the additional variable isn’t significant enough.\n",
    "For comparing the accuracy among different linear regression models, RMSE is a better choice than R Squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd201e-1845-4fee-8a26-515e0e2e8386",
   "metadata": {},
   "source": [
    "# Q5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2307eb-5b32-4141-b903-aa4faeb63b11",
   "metadata": {},
   "source": [
    "RMSE has the benefit of penalizing large errors more so can be more appropriate in some cases, for example, if being off by 10 is more than twice as bad as being off by 5. But if being off by 10 is just twice as bad as being off by 5, then MAE is more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be810a6-48d9-4207-be0f-d3ca0097a3e6",
   "metadata": {},
   "source": [
    "# Q6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3745aa-5adb-47a9-9a86-98c027141b91",
   "metadata": {},
   "source": [
    "similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square. Ridge regression is also referred to as L2 Regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6830c9c-be41-4cca-a87a-183e6cdc1d16",
   "metadata": {},
   "source": [
    "# Q7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92ad28a-9ff5-4db2-aa2b-03e271c4d16e",
   "metadata": {},
   "source": [
    "In short, Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737bf9e2-d38b-45de-9531-4f9aece2c699",
   "metadata": {},
   "source": [
    "# Q8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22681156-fe36-417d-a827-7822b3f0755d",
   "metadata": {},
   "source": [
    "Since linear regression assumes a linear relationship between the input and output varaibles, it fails to fit complex datasets properly. In most real life scenarios the relationship between the variables of the dataset isn't linear and hence a straight line doesn't fit the data properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5842b2-5bd7-43b9-9a23-a9112a3f8670",
   "metadata": {},
   "source": [
    "# Q9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3cbbb5-965f-4210-acb5-508bbe158f81",
   "metadata": {},
   "source": [
    "Both metrics have comparable behaviour in response to model bias and asymptote to the model bias as the bias increases. MAE is shown to be an unbiased estimator while RMSE is a biased estimator. MAE also has a lower sample variance compared with RMSE indicating MAE is the most robust choice.\n",
    "Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE is most useful when large errors are particularly undesirable.\n",
    "\n",
    "The MAE and the RMSE can be used together to diagnose the variation in the errors in a set of forecasts. The RMSE will always be larger or equal to the MAE; the greater difference between them, the greater the variance in the individual errors in the sample. If the RMSE=MAE, then all the errors are of the same magnitude\n",
    "\n",
    "Both the MAE and RMSE can range from 0 to ∞. They are negatively-oriented scores: Lower values are better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc05bc-0431-4938-834a-71769da77ece",
   "metadata": {},
   "source": [
    "# Q10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb4a5a-5b61-4289-893c-351f6e4ae3d2",
   "metadata": {},
   "source": [
    "What is the limitation of regularization?\n",
    "Cons of Regularization\n",
    "\n",
    "Regularization leads to dimensionality reduction, which means the machine learning model is built using a lower dimensional dataset. This generally leads to a high bias errror. If regularization is performed before training the model, a perfect balance between bias-variance tradeoff must be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfc2bbf-2a69-495c-a987-ec6dd79d5b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
